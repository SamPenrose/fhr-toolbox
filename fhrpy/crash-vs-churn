#!/usr/bin/env python
"""
Analyze a range of dates for new and lost Firefox profiles.
"""

import healthreportutils
from datetime import date, datetime, timedelta
import csv, os, shutil, sys, tempfile, time

import mrjob
from mrjob.job import MRJob

try:
    import simplejson as json
except ImportError:
    import json

DAYS_PER_WEEK = 7
TOTAL_DAYS = 168

# How many weeks must a user be gone to be considered "lost"?
LAG_WEEKS = 4

TOTAL_WEEKS = TOTAL_DAYS / DAYS_PER_WEEK

main_channels = (
    'nightly',
    'aurora',
    'beta',
    'release'
)


def start_date(dstr):
    """
    Measure Sunday-Saturday, for no particularly good reason.
    """
    snapshot = datetime.strptime(dstr, "%Y-%m-%d").date()
    startdate = healthreportutils.last_saturday(snapshot)
    return startdate

def date_back(start, days, skip=1):
    """iter backwards from start for N days"""
    date = start
    for n in xrange(0, days, skip):
        yield date - timedelta(days=n)

def active_day(day):
    if day is None:
        return False
    return any(k != "org.mozilla.crashes.crashes" for k in day)

def date_from_epoch_day(number):
    timestamp = number * 24 * 3600
    return date.fromtimestamp(timestamp)

def get_creation_date(payload, start, end):
    days_since_epoch = payload.get('data', {}).get('last', {}).get(
            'org.mozilla.profile.age', {}).get('profileCreation')
    if days_since_epoch is None:
        return ('pcd-missing', None)
    try:
        days_since_epoch = int(days_since_epoch)
        creation_date = date_from_epoch_day(days_since_epoch)
    except Exception:
        return ('pcd-corrupt', None)

    if creation_date < start:
        if (creation_date + timedelta(7)) > start:
            return ('creation-early-1-week', None)
        else:
            return ('creation-early-more', None)
    if creation_date > end:
        return ('creation-late', None)

    return (None, creation_date)


def get_crashes(day_dict):
    crashes = day_dict.get('org.mozilla.crashes.crashes', None)
    if crashes is None:
        return 0
    count = 0
    for k, v in crashes.items():
        if k == '_v':
            continue
        if 'submission' in k:
            continue
        try:
            value = int(v)
        except Exception:
            continue
        else:
            count += value
    return count


def least_squares(y):
    N = len(y)
    x = range(N)
    regressor = ((sum(x[i] * y[i] for i in xrange(N)) - 1.0/N*sum(x)*sum(y))
         /
         (sum(x[i]**2 for i in xrange(N)) - 1.0/N*sum(x)**2))
    offset = 1.*sum(y)/N - regressor * 1.*sum(x)/N
    return offset, regressor


@healthreportutils.FHRMapper()
def map(job, key, payload):
    # Use dataset-window to determine. Note "start_date" misnomer.
    window_end = datetime.strptime(job.options.start_date, "%Y-%m-%d").date()
    window_start = window_end - timedelta(180)
    weekly_activity = {}
    next_week = window_start
    churn_window = []
    while next_week.weekday() != 6:
        next_week -= timedelta(1)
    while next_week < window_end:
        weekly_activity[next_week.isoformat()] = []
        churn_window.append(next_week.isoformat())
        next_week += timedelta(7)
    churn_window = churn_window[-5:]

    problem, creation_date = get_creation_date(payload, window_start, window_end)
    if problem:
        yield (problem, 1)
        return

    day_dict = payload.get('data', {}).get('days', {})
    days = day_dict.keys()
    days = [(day, get_crashes, day_dict[day]) for day in days]

    current = payload.get('data', {}).get('last', {}).get(
        'org.mozilla.appSessions.current', {})
    current_day = current.get('startDay')
    if current_day:
        try:
            current_day = date_from_epoch_day(int(current_day))
        except Exception:
            yield ('bad-day-value', 1)
        else:
            days.append((current_day.isoformat(), get_crashes, current))
    days.sort()

    # We define churn as whether we have an empty list for the value of
    # weekly_activity[d] for all d in the last four Sundays.
    churned = True
    for day, getter, parameter in days:
        if day < window_start.isoformat():
            continue
        if day > window_end.isoformat():
            break
        # XXX compare day to window
        crashes = getter(parameter)
        sunday = day
        while sunday not in weekly_activity:
            sunday = (datetime.strptime(sunday, "%Y-%m-%d").date() - timedelta(1)).isoformat()
        weekly_activity[sunday].append(crashes)
        if sunday in churn_window:
            churned = False

    results = []
    total = 0.0
    last_regressor = None
    creation_boundary = (creation_date + timedelta(7)).isoformat()
    for week, activity in weekly_activity.items():
        # Strip leading weeks before PCD.
        # This should be integrated with previous loop.
        if week < creation_boundary:
            continue
        regressor = None
        if len(activity) > 1:
            _, regressor = least_squares(activity)
            last_regressor = regressor
        weekly = sum(activity)
        total += weekly
        results.append((week, weekly))

    prefix = 'churned' if churned else 'nochurn'
    if not total:
        yield ('%s-sans-activity' % prefix, 1)
        return
    if total == 1.0:
        yield ('%s-exactly-one-activity' % prefix, 1)
    else:
        yield ('%s-with-activity' % prefix, 1)
    yield ('%s-active-days' % prefix, len(days))

    results.sort()

    # regress last weeks of activity
    last = results[-1][1]
    while not last:
        results.pop()
        last = results[-1][1]

    yield (prefix+'-count', 1)
    yield (prefix+'-ztotal', total)
    if last_regressor is not None:
        yield (prefix+'-last-week-regressor', last_regressor)
    if results:
        try:
            _, weekly_regressor = least_squares([t[1] for t in results])
            yield (prefix+'-weekly-regressor', weekly_regressor)
        except Exception:
            yield ('weekly-regressor-error', 1)
        try:
            _, three_regressor = least_squares([t[1] for t in results[-4:]])
            yield (prefix+'-three-week-regressor', three_regressor)
        except Exception:
            yield ('three-week-regressor-error', 1)


def deciles(l, width, cleaner=None):
    if width < 100:
        return None
    l.sort()
    step = width / 10.0
    result = []
    for i in range(1, 10):
        index = int(round(step*i))
        value = l[index]
        if cleaner:
            value = cleaner(value)
        result.append(value)
    return '*'.join(str(s) for s in result)

def reduce(job, k, vlist):
    vlist = list(vlist)
    vlist.sort()
    total = sum(vlist)
    length = len(vlist)
    def cleaner(v):
        return round(v, 2)
    if k == 'active-hours':
        yield (k+'-deciles', deciles(vlist, length, cleaner))
        yield (k+'-average', total/float(length))
        yield (k+'-count', length)
    if k.endswith('regressor'):
        yield (k+'-deciles', deciles(vlist, length, cleaner))
        yield (k, total/float(length))
    else:
        yield (k, total)

class AggJob(MRJob):
    HADOOP_INPUT_FORMAT="org.apache.hadoop.mapred.SequenceFileAsTextInputFormat"
    INPUT_PROTOCOL = mrjob.protocol.RawProtocol

    def run_job(self):
        self.stdout = tempfile.TemporaryFile()

        if self.options.start_date is None:
            raise Exception("--start-date is required")
        # validate the start date here
        start_date(self.options.start_date)

        # Do the big work
        super(AggJob, self).run_job()

        # Produce the separated output files
        outpath = self.options.output_path
        if outpath is None:
            outpath = os.path.expanduser(("~/%s-%s" % (os.path.basename(__file__), int(time.time()))))
        output(self.stdout, outpath)
        print >> sys.stderr, "Output:", outpath

    def configure_options(self):
        super(AggJob, self).configure_options()

        self.add_passthrough_option('--output-path', help="Specify output path",
                                    default=None)
        self.add_passthrough_option('--start-date', help="Specify start date",
                                    default=None)

    def mapper(self, key, value):
        return map(self, key, value)

    def reducer(self, key, vlist):
        return reduce(self, key, vlist)

    # combiner = reducer

def getresults(fd):
    fd.seek(0)
    for line in fd:
        k, v = line.split("\t")
        yield json.loads(k), json.loads(v)

def unwrap(l, v):
    """
    Unwrap a value into a list. Dicts are added in their repr form.
    """
    if isinstance(v, (tuple, list)):
        for e in v:
            unwrap(l, e)
    elif isinstance(v, dict):
        l.append(repr(v))
    elif isinstance(v, unicode):
        l.append(v.encode("utf-8"))
    else:
        l.append(v)

def output(fd, path):
    try:
        shutil.rmtree(path)
    except OSError:
        pass
    os.mkdir(path)

    writers = {}
    for k, v in getresults(fd):
        l = []
        unwrap(l, k)
        unwrap(l, v)
        fname = l.pop(0)
        if fname in writers:
            w = writers[fname]
        else:
            fd = open(os.path.join(path, str(fname) + ".csv"), "w")
            w = csv.writer(fd)
            writers[fname] = w
        w.writerow(l)

if __name__ == '__main__':
    AggJob.run()
