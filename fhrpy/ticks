#!/usr/bin/env python
"""
Analyze a range of dates for new and lost Firefox profiles.
"""

import healthreportutils
from datetime import date, datetime, timedelta
import os, shutil, csv
import codecs

import mrjob
from mrjob.job import MRJob
import tempfile

try:
    import simplejson as json
except ImportError:
    import json

DAYS_PER_WEEK = 7
TOTAL_DAYS = 168

# How many weeks must a user be gone to be considered "lost"?
LAG_WEEKS = 4

TOTAL_WEEKS = TOTAL_DAYS / DAYS_PER_WEEK

main_channels = (
    'nightly',
    'aurora',
    'beta',
    'release'
)


def start_date(dstr):
    """
    Measure Sunday-Saturday, for no particularly good reason.
    """
    snapshot = datetime.strptime(dstr, "%Y-%m-%d").date()
    startdate = healthreportutils.last_saturday(snapshot)
    return startdate

def date_back(start, days, skip=1):
    """iter backwards from start for N days"""
    date = start
    for n in xrange(0, days, skip):
        yield date - timedelta(days=n)

def active_day(day):
    if day is None:
        return False
    return any(k != "org.mozilla.crashes.crashes" for k in day)

def get_creation_date(payload):
    days_since_epoch = payload.get('data', {}).get('last', {}).get(
            'org.mozilla.profile.age', {}).get('profileCreation')
    if days_since_epoch is None:
        return ('missing', None)
    days_since_epoch = int(days_since_epoch)
    try:
        timestamp = days_since_epoch * 24 * 3600
        creation_date = date.fromtimestamp(timestamp)
    except Exception:
        return ('corrupt', None)
    return (None, creation_date)

@healthreportutils.FHRMapper()
def map(job, key, payload):
    days = payload.get('data', {}).get('days', {})
    days_count = len(days)
    if days_count > 9:
        days_count = "big"
    yield ('days-count-%s' % days_count, 1)

    current = payload.get('data', {}).get('last', {}).get(
        'org.mozilla.appSessions.current', {}).get('activeTicks')
    if current:
        yield ('current-active-hours', round((current*5.0) / 3600, 2))
        # yield ('has-current', 1)
    yield ('has_current-%d-has_days-%d'	% (int(bool(current)), int(bool(days))), 1)

    # day = datetime.strptime(days.keys()[0], "%Y-%m-%d").date()
    # yield ((day - creation_date).days, 1)
    # return
    ticks = 0
    count = 0
    days_with = 0
    days_sans = 0
    for day in days:
        activity = days[day].get('org.mozilla.appSessions.previous', None)
        if activity is None:
            days_sans += 1
            continue
        sessions = len(activity.get('main', []))
        cleanTime = activity.get('cleanTotalTime', [])
        cleanTicks = activity.get('cleanActiveTicks', [])
        # if len(cleanTime) != len(cleanTicks):
        #     yield ('time-tick-mismatch',
        #            (len(clean), len(activity.get('cleanActiveTicks', []))))

        abortedTicks = activity.get('abortedActiveTicks', [])
        if (cleanTicks or abortedTicks):
            days_with += 1
        else:
            days_sans += 1

        ticks += sum(cleanTicks)
        ticks += sum(abortedTicks)
    # print ticks,
    hours = (ticks*5.0) / 3600
    if hours > 2.0:
        hours = int(round(hours))
    elif hours == 0.0:
        hours = 0
    else:
        hours = round(hours, 2)
    yield ('active-hours', hours)
    yield ('days-with', days_with)
    yield ('days-sans', days_sans)
    if days:
        if days_with:
            yield ('some-days-with', 1)
        else:
            yield ('all-days-empty', 1)

def deciles(l, width):
    if width < 100:
        return None
    l.sort()
    step = width / 10.0
    result = []
    for i in range(10):
        index = int(round(step*i))
        result.append(l[index])
    return '*'.join(str(s) for s in result)

def reduce(job, k, vlist):
    vlist = list(vlist)
    vlist.sort()
    total = sum(vlist)
    length = len(vlist)
    if k == 'active-hours':
        yield (k+'-deciles', deciles(vlist, length))
        yield (k+'-average', total/float(length))
        yield (k+'-count', length)
    yield (k, total)

class AggJob(MRJob):
    HADOOP_INPUT_FORMAT="org.apache.hadoop.mapred.SequenceFileAsTextInputFormat"
    INPUT_PROTOCOL = mrjob.protocol.RawProtocol

    def run_job(self):
        self.stdout = tempfile.TemporaryFile()

        if self.options.start_date is None:
            raise Exception("--start-date is required")
        # validate the start date here
        start_date(self.options.start_date)

        # Do the big work
        super(AggJob, self).run_job()

        # Produce the separated output files
        outpath = self.options.output_path
        if outpath is None:
            outpath = os.path.expanduser(("~/%s-" % os.path.basename(__file__)) + self.options.start_date)
        output(self.stdout, outpath)

    def configure_options(self):
        super(AggJob, self).configure_options()

        self.add_passthrough_option('--output-path', help="Specify output path",
                                    default=None)
        self.add_passthrough_option('--start-date', help="Specify start date",
                                    default=None)

    def mapper(self, key, value):
        return map(self, key, value)

    def reducer(self, key, vlist):
        return reduce(self, key, vlist)

    # combiner = reducer

def getresults(fd):
    fd.seek(0)
    for line in fd:
        k, v = line.split("\t")
        yield json.loads(k), json.loads(v)

def unwrap(l, v):
    """
    Unwrap a value into a list. Dicts are added in their repr form.
    """
    if isinstance(v, (tuple, list)):
        for e in v:
            unwrap(l, e)
    elif isinstance(v, dict):
        l.append(repr(v))
    elif isinstance(v, unicode):
        l.append(v.encode("utf-8"))
    else:
        l.append(v)

def output(fd, path):
    try:
        shutil.rmtree(path)
    except OSError:
        pass
    os.mkdir(path)

    writers = {}
    for k, v in getresults(fd):
        l = []
        unwrap(l, k)
        unwrap(l, v)
        fname = l.pop(0)
        if fname in writers:
            w = writers[fname]
        else:
            fd = open(os.path.join(path, str(fname) + ".csv"), "w")
            w = csv.writer(fd)
            writers[fname] = w
        w.writerow(l)

if __name__ == '__main__':
    AggJob.run()
